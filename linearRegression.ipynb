{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [머신러닝 강의 13] Linear Regression (I)\n",
    "\n",
    "\n",
    "\n",
    "### 회귀 (Regression)\n",
    "\n",
    "* Training Data를 이용하여 데이터의 특성과 상관관계 등을 파악하고, 그 결과를 바탕으로 Training Data에 없는 미지의 데이터가 주어졌을 경우에, 그 결과를 연속적인 (숫자)값으로 예측 하는 것\n",
    "    - (예) 공부시간과 시험 성적 관계, 집 평수와 집 가격 관계 등\n",
    "    ![image](https://user-images.githubusercontent.com/63198352/89430252-845e2b00-d779-11ea-9586-4a0748da291f.png)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regression 학습 개념\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63198352/89430698-f9c9fb80-d779-11ea-8fe3-0a304d093f4c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오차(error) 가중치(weight) W, 바이어스(bias) b\n",
    "\n",
    "* 오차가 크다면, 임의로 설정한 직선의 가중치와 바이어스 값이 잘못된 것, 오차가 작다면 잘된것, \n",
    "    - 오차가 작다면 미래 값 예측도 정확할 수 있다고 예상할 수 있음\n",
    "    \n",
    "* 머신러닝의 Regression 시스템은 모든 데이터의 오차의 합이 최소가 되서, 미래 값을 잘 예측할 수 있는 가중치 W와 바이어스 b 값을 찾아야 한다.\n",
    "![image](https://user-images.githubusercontent.com/63198352/89430892-34cc2f00-d77a-11ea-8533-c32dc4b7056b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손실함수 loss function\n",
    "\n",
    "* 손실함수는 training data의 정답(t)와 입력(x)에 대한 계산값 y의 차이를 모두 더해 수식으로 나타낸 것이다\n",
    "\n",
    "* 각각의 오차인 t - y 를 모두 더해서 손실함수를 구할 시 각각의 오차가 (+) , (-) 등으로 존재하기 때문에 오차의 합이 0으로 나올 수 도 있다. \n",
    "    - 0이 최소 오차 값인지 아닌지 판별이 어렵다\n",
    "    \n",
    "* 때문에 손실함수에서 오차를 계산할 때는 제곱을 사용한다. 그래서 오차는 언제나 양수이며, 제곱을 하기 때문에 정답과 계산값 차이가 크다면, 제곱에 의해 오차는 더 큰 값을 가지게 되고, 머신러닝 학습에 있어 장점을 가진다\n",
    "\n",
    "**손실함수라는 것은 즉 모든 데이터에 대한 평균 오차값을 나타낸다**\n",
    "\n",
    "x와 t는 training data에서 주어진 값이므로, 손실함수인 E(W, b)는 결국 W와 b에 의해 영향을 받게되는 함수임을 알 수 있다. \n",
    "\n",
    "## 이처럼 training data를 바탕으로 손실함수 E(W, b)가 최소값을 갖도록 하는 (W, b)를 구하는 것이 linear regression model 의 최종 목적이다\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63198352/89431171-870d5000-d77a-11ea-9135-564f0f6aaf61.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63198352/89431431-e2d7d900-d77a-11ea-959d-b307d44eefe6.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63198352/89431517-06028880-d77b-11ea-9b8a-83ac2d8bc326.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [머신러닝 강의 14] Linear Regression (II)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법 (gradient decent algorithm)\n",
    "\n",
    "* 손실함수는 오차의 평균값을 나타내기 때문에 손실함수가 최소값을 갖는다는 것은?\n",
    "    - 실제 정답과 계산 값의 차이인 오차가 최소가 되어 미지의 데이터에 대해 결과를 더 잘 예측 할 수 있다는 것을 의미\n",
    "    \n",
    "* 이러한 손실함수는 W, b에 영향을 받기 때문에, 손실함수가 최소가 되는 가중치 W와 바이어스 b를 찾는 것이 regression을 구현하는 최종 목효\n",
    "\n",
    "* 이처럼 W에서의 직선의 기울기인 미분 값을 이용하여 그 값이 작아지는 방향으로 진행하여 손실함수 최소값을 찾는 방법을 경사하강법이라 함\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63198352/89431931-7f01e000-d77b-11ea-9c34-6b82d70f383d.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63198352/89432219-d011d400-d77b-11ea-8e9f-0ed0ee167fd8.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63198352/89432364-fc2d5500-d77b-11ea-941a-777bb46d5490.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63198352/89432487-267f1280-d77c-11ea-8d6f-d4b6bb6a2c19.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63198352/89433129-e66c5f80-d77c-11ea-9001-9959f8dcb7f0.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63198352/89433218-069c1e80-d77d-11ea-9449-981560d2d160.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63198352/89433260-14ea3a80-d77d-11ea-92c7-e7f9004e3400.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [머신러닝 강의 15] Linear Regression (III)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63198352/89433491-5844a900-d77d-11ea-9dff-c3a7221dd93d.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple regression concept \n",
    "\n",
    "* 오차를 계싼하기 위해 training data의 모든 입력 x에 대해 각각의 y = Wx + b를 계산해야 한다\n",
    "    - 이때 입력 x , 정답 t, 가중치 W 모두를 행렬로 나타낸 후\n",
    "    - 행렬 곱(dot product)을 이용\n",
    "    - 계산 값 y 또한 행렬로 표시되게 끔하여 모든 입력 데이터에 대해 한번에 쉽게 계산된다\n",
    "\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63198352/89433563-71e5f080-d77d-11ea-85c5-c80147fa1a34.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63198352/89433958-ecaf0b80-d77d-11ea-92a2-e2d2d7e04fe2.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63198352/89434013-ff294500-d77d-11ea-90ba-75e89e52bbb4.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63198352/89434089-18ca8c80-d77e-11ea-8199-b17348585635.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multi-variable regession\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63198352/89434201-38fa4b80-d77e-11ea-9157-afd1ba221841.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63198352/89434422-7bbc2380-d77e-11ea-8045-65377df81d9f.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63198352/89434532-9a221f00-d77e-11ea-92fb-1abadc0a99e2.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63198352/89434565-a60de100-d77e-11ea-8031-8434dd618e5c.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/63198352/89434656-c178ec00-d77e-11ea-8c27-b976757be10d.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QQQQQ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
